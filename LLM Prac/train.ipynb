{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n",
      "d:\\CourseworkFolder\\DPSynthData\\LLM PRAC\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import json\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from torchtune.modules import RotaryPositionalEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "#block_size = 128\n",
    "max_iters = 3000\n",
    "eval_interval = 100\n",
    "learning_rate = 6e-4\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.1\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 96341 timelines. Max length: 29917, Average length: 200.0\n",
      "Val set: 10705 timelines. Max length: 25961, Average length: 188.3\n",
      "Vocab size: 12666\n"
     ]
    }
   ],
   "source": [
    "train_data = torch.load(\"train.pt\")\n",
    "val_data = torch.load(\"val.pt\")\n",
    "\n",
    "def find_max_len(dataset, name=\"Dataset\"):\n",
    "    max_len = 0\n",
    "    total = 0\n",
    "    count = 0\n",
    "    for seq in dataset:\n",
    "        seq_len = len(seq)\n",
    "        if seq_len > max_len:\n",
    "            max_len = seq_len\n",
    "        total += seq_len\n",
    "        count += 1\n",
    "    avg_len = total / count if count > 0 else 0\n",
    "    print(f\"{name}: {count} timelines. Max length: {max_len}, Average length: {avg_len:.1f}\")\n",
    "\n",
    "find_max_len(train_data, \"Train set\")\n",
    "find_max_len(val_data, \"Val set\")\n",
    "\n",
    "with open(\"D:/CourseworkFolder/DPSynthData/Data Manipulation/token_map.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    token_map = json.load(f)\n",
    "vocab_size = len(token_map)\n",
    "print(f\"Vocab size: {vocab_size}\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    batch = random.sample(data, batch_size)\n",
    "    x_batch = [torch.tensor(seq[:-1], dtype=torch.long, device=device) for seq in batch]\n",
    "    y_batch = [torch.tensor(seq[1:], dtype=torch.long, device=device) for seq in batch]\n",
    "    return x_batch, y_batch\n",
    "\n",
    "@torch.no_grad()\n",
    "'''\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            x_batch, y_batch = get_batch(split)\n",
    "            \n",
    "            batch_losses = []\n",
    "            for x, y in zip(x_batch, y_batch):\n",
    "                logits, loss = model([x], [y])  \n",
    "                batch_losses.append(loss.item())\n",
    "            \n",
    "            avg_loss = sum(batch_losses) / len(batch_losses)\n",
    "            losses[k] = avg_loss\n",
    "        \n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "    '''\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            _, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, n_embd):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = n_embd // num_heads\n",
    "        assert n_embd % num_heads == 0, \"Embedding size must be divisible by num_heads\"\n",
    "\n",
    "        self.query = nn.Linear(n_embd, n_embd, bias=False)\n",
    "        self.key = nn.Linear(n_embd, n_embd, bias=False)\n",
    "        self.value = nn.Linear(n_embd, n_embd, bias=False)\n",
    "        \n",
    "        \n",
    "        self.rope = RotaryPositionalEmbeddings(dim=self.head_dim, max_seq_len=50000)\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape  # (batch_size, sequence_length, embedding_dim)\n",
    "        \n",
    "        # Linear projections and reshape to (B, T, n_head, head_dim)\n",
    "        q = self.query(x).view(B, T, self.num_heads, self.head_dim)\n",
    "        k = self.key(x).view(B, T, self.num_heads, self.head_dim)\n",
    "        v = self.value(x).view(B, T, self.num_heads, self.head_dim)\n",
    "        \n",
    "        #print(f\"q.shape: {q.shape}\")\n",
    "        \n",
    "        # Apply RoPE to q and k (automatic positional embeddings)\n",
    "        q = self.rope(q)\n",
    "        k = self.rope(k)\n",
    "\n",
    "        # Scaled Dot-Product Attention with flash attention\n",
    "        out = F.scaled_dot_product_attention(\n",
    "            q, k, v,\n",
    "            attn_mask=None,\n",
    "            dropout_p=dropout if self.training else 0.0,\n",
    "            is_causal=True\n",
    "        )  # (B, T, n_head, head_dim)\n",
    "\n",
    "        # Merge heads: (B, T, n_head, head_dim) -> (B, T, C)\n",
    "        out = out.transpose(1, 2).reshape(B, T, C)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        self.sa = MultiHeadAttention(n_head, n_embd)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class TimelineLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        #self.position_embedding_table = nn.Embedding(12000, n_embd)  \n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, x_batch, y_batch=None):\n",
    "        logits_batch = []\n",
    "        losses = []\n",
    "\n",
    "        for x in x_batch:\n",
    "            #T = x.size(0)\n",
    "            tok_emb = self.token_embedding_table(x)\n",
    "            #pos_emb = self.position_embedding_table(torch.arange(T, device=x.device))\n",
    "            x_in = tok_emb \n",
    "            x_in = x_in.unsqueeze(0)\n",
    "\n",
    "            x_out = self.blocks(x_in)\n",
    "            x_out = self.ln_f(x_out)\n",
    "            logits = self.lm_head(x_out).squeeze(0)  \n",
    "            logits_batch.append(logits)\n",
    "\n",
    "        if y_batch is not None:\n",
    "            for logits, y in zip(logits_batch, y_batch):\n",
    "                loss = F.cross_entropy(logits, y)\n",
    "                losses.append(loss)\n",
    "            return logits_batch, torch.stack(losses).mean()\n",
    "\n",
    "        return logits_batch, None\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        min_tokens = 1\n",
    "        eos_token = 12665\n",
    "\n",
    "        for i in range(max_new_tokens):\n",
    "            T = idx.size(1)\n",
    "            tok_emb = self.token_embedding_table(idx)\n",
    "            #pos_emb = self.position_embedding_table(torch.arange(T, device=idx.device))\n",
    "            x = tok_emb\n",
    "            x = self.blocks(x)\n",
    "            x = self.ln_f(x)\n",
    "            logits = self.lm_head(x)\n",
    "\n",
    "            probs = F.softmax(logits[:, -1, :], dim=-1)\n",
    "\n",
    "            if i < min_tokens:\n",
    "                probs[:, eos_token] = 0\n",
    "                probs = probs / probs.sum(dim=-1, keepdim=True)\n",
    "\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, next_token), dim=1)\n",
    "         \n",
    "            if next_token.item() == eos_token and i >= min_tokens:\n",
    "                break\n",
    "\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Starting training\n",
      " Model has 1.83M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Model:   0%|          | 0/3000 [00:00<?, ?steps/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Step 0: train loss 9.6205, val loss 9.6189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Model:   3%|▎         | 100/3000 [01:27<22:08,  2.18steps/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Step 100: train loss 4.1366, val loss 4.1614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Model:   7%|▋         | 200/3000 [02:48<14:04,  3.32steps/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Step 200: train loss 3.0425, val loss 3.0348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Model:  10%|█         | 300/3000 [04:08<11:25,  3.94steps/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Step 300: train loss 2.7319, val loss 2.7297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Model:  13%|█▎        | 400/3000 [05:22<09:58,  4.35steps/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Step 400: train loss 2.5987, val loss 2.5974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Model:  17%|█▋        | 500/3000 [06:39<11:25,  3.65steps/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Step 500: train loss 2.5355, val loss 2.5086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Model:  20%|██        | 600/3000 [07:53<12:50,  3.11steps/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Step 600: train loss 2.4747, val loss 2.4821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Model:  23%|██▎       | 700/3000 [09:11<12:38,  3.03steps/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Step 700: train loss 2.4511, val loss 2.4382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Model:  27%|██▋       | 800/3000 [10:31<08:08,  4.51steps/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Step 800: train loss 2.4102, val loss 2.3974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Model:  30%|███       | 900/3000 [11:50<17:14,  2.03steps/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Step 900: train loss 2.3800, val loss 2.3944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Model:  33%|███▎      | 1000/3000 [13:05<07:46,  4.29steps/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Step 1000: train loss 2.3790, val loss 2.3610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Model:  37%|███▋      | 1100/3000 [14:21<07:58,  3.97steps/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Step 1100: train loss 2.3196, val loss 2.3260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Model:  40%|████      | 1200/3000 [15:41<06:39,  4.50steps/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Step 1200: train loss 2.3245, val loss 2.3068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Model:  43%|████▎     | 1300/3000 [17:00<07:40,  3.69steps/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Step 1300: train loss 2.2987, val loss 2.2894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Model:  47%|████▋     | 1400/3000 [18:19<06:01,  4.42steps/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Step 1400: train loss 2.2666, val loss 2.2653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Model:  50%|█████     | 1500/3000 [19:39<05:54,  4.23steps/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Step 1500: train loss 2.2455, val loss 2.2484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Model:  53%|█████▎    | 1600/3000 [20:55<05:14,  4.45steps/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Step 1600: train loss 2.2353, val loss 2.2150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Model:  57%|█████▋    | 1700/3000 [22:14<04:58,  4.36steps/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Step 1700: train loss 2.2020, val loss 2.1891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Model:  60%|██████    | 1800/3000 [23:26<04:54,  4.07steps/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Step 1800: train loss 2.1809, val loss 2.1728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Model:  63%|██████▎   | 1900/3000 [24:44<05:27,  3.36steps/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Step 1900: train loss 2.1722, val loss 2.1625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Model:  67%|██████▋   | 2000/3000 [26:02<04:12,  3.96steps/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Step 2000: train loss 2.1536, val loss 2.1532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Model:  70%|███████   | 2100/3000 [27:26<03:54,  3.84steps/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Step 2100: train loss 2.1296, val loss 2.1407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Model:  73%|███████▎  | 2200/3000 [28:44<03:10,  4.20steps/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Step 2200: train loss 2.1203, val loss 2.1174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Model:  77%|███████▋  | 2300/3000 [30:01<02:37,  4.44steps/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Step 2300: train loss 2.1255, val loss 2.1142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Model:  80%|████████  | 2400/3000 [31:20<02:09,  4.63steps/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Step 2400: train loss 2.0831, val loss 2.1215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Model:  83%|████████▎ | 2500/3000 [32:33<01:47,  4.67steps/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Step 2500: train loss 2.0933, val loss 2.0859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Model:  87%|████████▋ | 2600/3000 [33:49<01:32,  4.34steps/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Step 2600: train loss 2.0725, val loss 2.0479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Model:  90%|█████████ | 2700/3000 [35:04<01:05,  4.55steps/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Step 2700: train loss 2.0645, val loss 2.0559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Model:  93%|█████████▎| 2800/3000 [36:23<00:49,  4.03steps/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Step 2800: train loss 2.0430, val loss 2.0384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Model:  97%|█████████▋| 2900/3000 [37:39<00:24,  4.10steps/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Step 2900: train loss 2.0442, val loss 2.0338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Model: 100%|██████████| 3000/3000 [38:55<00:00,  1.28steps/s]\n"
     ]
    }
   ],
   "source": [
    "print(\" Starting training\")\n",
    "model = TimelineLanguageModel().to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "print(f\" Model has {sum(p.numel() for p in model.parameters())/1e6:.2f}M parameters\")\n",
    "\n",
    "for iter in tqdm(range(max_iters), desc=\"Training Model\", unit=\"steps\"):\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\" Step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    xb, yb = get_batch(\"train\")\n",
    "    logits, loss = model(xb, yb)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Model saved.\n",
      " Generating synthetic timeline...\n",
      " Decoded Tokens:\n",
      "('Hospital Admission', 'admittime')\n",
      "admission_location=PHYSICIAN REFERRAL_discharge_location=HOME HEALTH CARE\n",
      "Diagnosis\n",
      "icd_code=470\n",
      "Diagnosis\n",
      "icd_code=600\n",
      "Diagnosis\n",
      "icd_code=781\n",
      "Diagnosis\n",
      "icd_code=V12\n",
      "icd_code=446\n",
      "Diagnosis\n",
      "icd_code=389\n",
      "__EOS__\n"
     ]
    }
   ],
   "source": [
    "with open(\"D:/CourseworkFolder/DPSynthData/Data Manipulation/token_map.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    token_map = json.load(f)\n",
    "itos = {v: k for k, v in token_map.items()}\n",
    "\n",
    "def decode(token_ids):\n",
    "    return [itos.get(i, f\"<unk:{i}>\") for i in token_ids]\n",
    "\n",
    "\n",
    "torch.save(model.state_dict(), \"timeline_model.pt\")\n",
    "print(\" Model saved.\")\n",
    "\n",
    "print(\" Generating synthetic timeline...\")\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "output = model.generate(context, max_new_tokens=300)\n",
    "decoded = decode(output[0].tolist())\n",
    "\n",
    "print(\" Decoded Tokens:\")\n",
    "for token in decoded:\n",
    "    print(token)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
